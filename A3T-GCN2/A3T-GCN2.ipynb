{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc57de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import torch\n",
    "import sklearn.metrics \n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from functools import partial, reduce\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric_temporal.nn.recurrent import A3TGCN2\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "\n",
    "from typing import Union, Tuple\n",
    "import math\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from torch_geometric_temporal.signal.static_graph_temporal_signal import StaticGraphTemporalSignal\n",
    "from torch_geometric_temporal.signal.dynamic_graph_temporal_signal import DynamicGraphTemporalSignal\n",
    "from torch_geometric_temporal.signal.dynamic_graph_static_signal import DynamicGraphStaticSignal\n",
    "\n",
    "from torch_geometric_temporal.signal.static_graph_temporal_signal_batch import StaticGraphTemporalSignalBatch\n",
    "from torch_geometric_temporal.signal.dynamic_graph_temporal_signal_batch import DynamicGraphTemporalSignalBatch\n",
    "from torch_geometric_temporal.signal.dynamic_graph_static_signal_batch import DynamicGraphStaticSignalBatch\n",
    "\n",
    "from torch_geometric_temporal.signal.static_hetero_graph_temporal_signal import StaticHeteroGraphTemporalSignal\n",
    "from torch_geometric_temporal.signal.dynamic_hetero_graph_temporal_signal import DynamicHeteroGraphTemporalSignal\n",
    "from torch_geometric_temporal.signal.dynamic_hetero_graph_static_signal import DynamicHeteroGraphStaticSignal\n",
    "\n",
    "from torch_geometric_temporal.signal.static_hetero_graph_temporal_signal_batch import StaticHeteroGraphTemporalSignalBatch\n",
    "from torch_geometric_temporal.signal.dynamic_hetero_graph_temporal_signal_batch import DynamicHeteroGraphTemporalSignalBatch\n",
    "from torch_geometric_temporal.signal.dynamic_hetero_graph_static_signal_batch import DynamicHeteroGraphStaticSignalBatch\n",
    "\n",
    "Discrete_Signal = Union[\n",
    "    StaticGraphTemporalSignal,\n",
    "    StaticGraphTemporalSignalBatch,\n",
    "    DynamicGraphTemporalSignal,\n",
    "    DynamicGraphTemporalSignalBatch,\n",
    "    DynamicGraphStaticSignal,\n",
    "    DynamicGraphStaticSignalBatch,\n",
    "    StaticHeteroGraphTemporalSignal,\n",
    "    StaticHeteroGraphTemporalSignalBatch,\n",
    "    DynamicHeteroGraphTemporalSignal,\n",
    "    DynamicHeteroGraphTemporalSignalBatch,\n",
    "    DynamicHeteroGraphStaticSignal,\n",
    "    DynamicHeteroGraphStaticSignalBatch,\n",
    "]\n",
    "import random\n",
    "\n",
    "# GPU support\n",
    "DEVICE = torch.device('cuda') # cuda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b32f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COVID19Dataloader(object):\n",
    "\n",
    "\n",
    "    def __init__(self, raw_data_dir=os.path.join(os.getcwd(), \"data_covid_weekly\")):\n",
    "        super(COVID19Dataloader, self).__init__()\n",
    "        self.raw_data_dir = raw_data_dir\n",
    "        self._read_web_data()\n",
    "\n",
    "    def _download_url(self, url, save_path):  # pragma: no cover\n",
    "        with urllib.request.urlopen(url) as dl_file:\n",
    "            with open(save_path, \"wb\") as out_file:\n",
    "                out_file.write(dl_file.read())\n",
    "\n",
    "    def _read_web_data(self):\n",
    "\n",
    "        A = np.load(os.path.join(self.raw_data_dir, \"adj_matrix_corr_adj_0.6.npy\"))\n",
    "        X = np.load(os.path.join(self.raw_data_dir, \"node_features_weekly_date.npy\")).transpose(\n",
    "            (1, 2, 0)\n",
    "        )\n",
    "        X_data = X.astype(np.float64)\n",
    "        X_dates = X.astype(np.float64)\n",
    "        \n",
    "        means = np.mean(X_data, axis=(0, 2))\n",
    "        X_data = X_data - means.reshape(1, -1, 1)\n",
    "        stds = np.std(X_data, axis=(0, 2))\n",
    "        X_data = X_data / stds.reshape(1, -1, 1)\n",
    "        \n",
    "        self.A = torch.from_numpy(A)   \n",
    "        self.X_data = torch.from_numpy(X_data)\n",
    "        self.X_dates = torch.from_numpy(X_dates)\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "\n",
    "    def _get_edges_and_weights(self):\n",
    "        edge_indices, values = dense_to_sparse(self.A)\n",
    "        edge_indices = edge_indices.numpy()\n",
    "        values = values.numpy()\n",
    "        self.edges = edge_indices\n",
    "        self.edge_weights = values\n",
    "        \n",
    "\n",
    "    def _generate_task(self, num_timesteps_in, num_timesteps_out):\n",
    "        \"\"\"Uses the node features of the graph and generates a feature/target\n",
    "        relationship of the shape\n",
    "        (num_nodes, num_node_features, num_timesteps_in) -> (num_nodes, num_timesteps_out)\n",
    "        predicting the COVID-19 weekly deaths using num_timesteps_in to predict the\n",
    "        deaths in the next num_timesteps_out\n",
    "\n",
    "        Args:\n",
    "            num_timesteps_in (int): number of timesteps the sequence model sees\n",
    "            num_timesteps_out (int): number of timesteps the sequence model has to predict\n",
    "        \"\"\"\n",
    "        indices = [\n",
    "            (i, i + (num_timesteps_in + num_timesteps_out))\n",
    "            for i in range(self.X_data.shape[2] - (num_timesteps_in + num_timesteps_out) + 1)\n",
    "        ]\n",
    "        \n",
    "\n",
    "        # Generate observations\n",
    "        features, target = [], []\n",
    "        dates = []\n",
    "        for i, j in indices:\n",
    "            features.append((self.X_data[:, 1:, i : i + num_timesteps_in]).numpy())\n",
    "            target.append((self.X_data[:, 1, i + num_timesteps_in : j]).numpy())\n",
    "            dates.append((self.X_dates[:, 0, i + num_timesteps_in : j]).numpy())\n",
    "\n",
    "        self.features = features\n",
    "        self.targets = target\n",
    "        self.dates = dates\n",
    "\n",
    "\n",
    "    def get_dataset(\n",
    "        self, num_timesteps_in, num_timesteps_out\n",
    "    ) -> StaticGraphTemporalSignal:\n",
    "        \"\"\"Returns data iterator for covid19 dataset as an instance of the\n",
    "        static graph temporal signal class.\n",
    "\n",
    "        Return types:\n",
    "            * **dataset** *(StaticGraphTemporalSignal)* - The COVID19\n",
    "                forecasting dataset.\n",
    "        \"\"\"\n",
    "        self._get_edges_and_weights()\n",
    "        self._generate_task(num_timesteps_in, num_timesteps_out)\n",
    "        dataset = StaticGraphTemporalSignal(\n",
    "            self.edges, self.edge_weights, self.features, self.targets\n",
    "        )\n",
    "\n",
    "        return dataset,self.dates,self.means,self.stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc450302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(past_values,future_values):\n",
    "    loader = COVID19Dataloader()\n",
    "    dataset,dates,means,stds = loader.get_dataset(past_values,future_values)\n",
    "    death_stds = stds[1]\n",
    "    death_mean = means[1]\n",
    "    \n",
    "    return dataset,dates,death_stds,death_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b25dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Number of samples / sequences: \",  len(tuple(dataset)))\n",
    "# print(next(iter(dataset))) # Show first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f7c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_signal_split_def(\n",
    "    data_iterator,dates,window, train_ratio,batch_size\n",
    ") -> Tuple[Discrete_Signal, Discrete_Signal]:\n",
    "    r\"\"\"Function to split a data iterator according to a fixed ratio.\n",
    "    Arg types:\n",
    "        * **data_iterator** *(Signal Iterator)* - Node features.\n",
    "        * **train_ratio** *(float)* - Graph edge indices.\n",
    "    Return types:\n",
    "        * **(train_iterator, test_iterator)** *(tuple of Signal Iterators)* - Train and test data iterators.\n",
    "    \"\"\"\n",
    "    #dataiterator = whole dataset\n",
    "    length = data_iterator.snapshot_count\n",
    "    train_snapshots = int(train_ratio * length)   \n",
    "    train_dataset = data_iterator[window:train_snapshots+window]\n",
    "    test_dataset = data_iterator[train_snapshots+window:train_snapshots+window+batch_size]\n",
    "    test_dates = dates[train_snapshots+window:train_snapshots+window+batch_size]\n",
    "    \n",
    "    for snapshot in train_dataset:\n",
    "        static_edge_index = snapshot.edge_index.to(DEVICE)\n",
    "        edge_weights = snapshot.edge_attr.to(DEVICE)\n",
    "        break;    \n",
    "    \n",
    "    train_input = np.array(train_dataset.features)\n",
    "    train_target = np.array(train_dataset.targets)\n",
    "\n",
    "    train_x_tensor = torch.from_numpy(train_input).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, P)\n",
    "    train_target_tensor = torch.from_numpy(train_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "\n",
    "    train_dataset_new = torch.utils.data.TensorDataset(train_x_tensor, train_target_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset_new, batch_size=batch_size, shuffle=False,drop_last=True)\n",
    "    \n",
    "    test_input = np.array(test_dataset.features)\n",
    "    test_target = np.array(test_dataset.targets)\n",
    "\n",
    "    test_x_tensor = torch.from_numpy(test_input).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, P)\n",
    "    test_target_tensor = torch.from_numpy(test_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "\n",
    "    test_dataset_new = torch.utils.data.TensorDataset(test_x_tensor, test_target_tensor)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset_new, batch_size=batch_size, shuffle=False,drop_last=True)\n",
    "    \n",
    "    return train_loader, test_loader,static_edge_index,edge_weights,test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af2471f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGNN(torch.nn.Module):\n",
    "    def __init__(self, node_features, periods, past_values,batch_size,hidden_units):\n",
    "        super(TemporalGNN, self).__init__()\n",
    "        # Attention Temporal Graph Convolutional Cell\n",
    "        self.tgnn = A3TGCN2(in_channels=node_features,  out_channels=hidden_units, periods=past_values, batch_size=batch_size\n",
    "                            ,add_self_loops=False)\n",
    "\n",
    "        self.linear = torch.nn.Linear(hidden_units, periods)\n",
    "       \n",
    "\n",
    "    def forward(self, x, edge_index,edge_weight):\n",
    "        \"\"\"\n",
    "        x = Node features for T time steps\n",
    "        edge_index = Graph edge indices\n",
    "        \"\"\"\n",
    "        h = self.tgnn(x, edge_index,edge_weight) # x [b, 49, 2, 15]  returns h [b, 49, 14]\n",
    "        h = F.relu(h) \n",
    "        h = self.linear(h)\n",
    "        \n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d0af995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(node_f,periods,batch_size,past_values,hidden_units,trainratio,test_ratio,epochs,dataset,dates):\n",
    "    model = TemporalGNN(node_features=node_f, periods=periods, past_values = past_values, batch_size=batch_size,hidden_units=hidden_units).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    samples = len(tuple(dataset))\n",
    "    pred_test = []\n",
    "\n",
    "    for i in tqdm(range(math.ceil(test_ratio*samples)-(batch_size))):\n",
    "\n",
    "        results = pd.DataFrame(columns = ['date'])\n",
    "        window = i +1\n",
    "\n",
    "        train_loader_d, test_loader_d, static_edge_index_d,edge_weights_d,test_dates = temporal_signal_split_def(dataset,dates, i, train_ratio=trainratio,\n",
    "                                                                                                                 batch_size=batch_size)\n",
    "        results['date'] = pd.to_datetime(test_dates[0][0], format='%Y%m%d')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()    \n",
    "            step = 0\n",
    "            loss_list = []\n",
    "            for encoder_inputs_train, labels_train in train_loader_d:\n",
    "\n",
    "                y_hat_train = model(encoder_inputs_train, static_edge_index_d,edge_weights_d) # Get model predictions\n",
    "\n",
    "                loss_train = loss_fn(y_hat_train, labels_train) # Mean squared error #loss = torch.mean((y_hat-labels)**2)  sqrt to change it to rmse\n",
    "                loss_train.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                step= step+ 1\n",
    "                loss_list.append(loss_train.item())\n",
    "                if step % 100 == 0 :\n",
    "                    print(sum(loss_list)/len(loss_list))\n",
    "    #         print(\"Epoch {} train MSE: {:.7f}\".format(epoch, sum(loss_list)/len(loss_list)))\n",
    "\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d66aef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_result(state, model):\n",
    "    pred_test = []\n",
    "    samples = len(tuple(dataset))\n",
    "    forecast_matrix = pd.DataFrame(columns = ['date'])\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    for i in tqdm(range(math.ceil(test_ratio*samples)-(batch_size))):\n",
    "        #print(results)\n",
    "        total_loss = []\n",
    "        results = pd.DataFrame(columns = ['date'])\n",
    "        window = i +1\n",
    "\n",
    "        train_loader_d, test_loader_d, static_edge_index_d,edge_weights_d,test_dates = temporal_signal_split_def(dataset,dates, i, train_ratio=trainratio,\n",
    "                                                                                                                 batch_size=batch_size)\n",
    "        results['date'] = pd.to_datetime(test_dates[0][0], format='%Y%m%d')\n",
    "\n",
    "        model.eval()\n",
    "        for encoder_inputs, labels in test_loader_d:\n",
    "            # Get model predictions\n",
    "            y_hat = model(encoder_inputs, static_edge_index_d,edge_weights_d)\n",
    "            \n",
    "            pred_test.append(y_hat[0,state,:])\n",
    "            \n",
    "            # Mean squared error\n",
    "            loss = loss_fn(y_hat, labels)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        results['window'+str(i)] = pred_test[i].cpu().detach().numpy()\n",
    "\n",
    "        if(i==0):\n",
    "            forecast_matrix = pd.merge(forecast_matrix, results, on=['date'], how='right')\n",
    "\n",
    "        else:\n",
    "            forecast_matrix = pd.merge(forecast_matrix, results, on=['date'], how='outer')\n",
    "    \n",
    "    return forecast_matrix\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db823174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast_matrix.set_index('date', inplace=True)\n",
    "def predictions(forecast_matrix):\n",
    "    forecast_matrix.set_index('date', inplace=True)\n",
    "    start = 0\n",
    "    end = 4\n",
    "    daily_pred = []\n",
    "    window = 0\n",
    "\n",
    "    for i in range(len(forecast_matrix.columns)-4):\n",
    "        forecast_matrix_temp = forecast_matrix.iloc[start:end]\n",
    "\n",
    "        date = forecast_matrix_temp.tail(1).index.item()\n",
    "        l_row = forecast_matrix_temp.tail(1)\n",
    "\n",
    "\n",
    "        daily_pred.append([date,l_row.iloc[0][window],l_row.iloc[0][window+1],l_row.iloc[0][window+2],l_row.iloc[0][window+3]])\n",
    "        window = window + 1\n",
    "        start = end \n",
    "        end = end+1    \n",
    "\n",
    "    daily_predictions = pd.DataFrame(daily_pred, columns=[ 'Date',str('week4_GNN'),str('week3_GNN'),\n",
    "                                                                  str('week2_GNN'),\n",
    "                                                                  str('week1_GNN')])\n",
    "    daily_predictions['Date'] = pd.to_datetime(daily_predictions['Date'])\n",
    "    daily_nn = daily_predictions\n",
    "    return daily_nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f34bd51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observed(state,daily_nn):\n",
    "   \n",
    "    data = pd.read_csv(\"state_weekly_data.csv\")\n",
    "#     data = data.loc[data['Province_State'] == \"Georgia\"]\n",
    "    data = data[[\"Date\",\"Province_State\", \"Deaths\",\"Confirmed\"]]\n",
    "    indexAge = data[(data['Province_State'] == 'Recovered') | (data['Province_State'] == 'Alaska') | (data['Province_State'] == 'Hawaii') | (data['Province_State'] == 'Puerto Rico') | (data['Province_State'] == 'Virgin Islands')\n",
    "                    | (data['Province_State'] == 'American Samoa') | (data['Province_State'] == 'Diamond Princess')| (data['Province_State'] == 'Grand Princess')| (data['Province_State'] == 'Guam')| (data['Province_State'] == 'Northern Mariana Islands')].index\n",
    "    data.drop(indexAge , inplace=True)\n",
    "    data.reset_index(inplace=True)\n",
    "    data = data.drop(columns = ['index'])\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    \n",
    "    state_to_number ={'Alabama':0, 'Arizona':1, 'Arkansas':2,\n",
    "       'California':3, 'Colorado':4, 'Connecticut':5, 'Delaware':6, 'District of Columbia':7, 'Florida':8, \n",
    "        'Georgia':9, 'Idaho':10, 'Illinois':11, 'Indiana':12, 'Iowa':13, 'Kansas':14, 'Kentucky':15, 'Louisiana':16,\n",
    "        'Maine':17, 'Maryland':18,'Massachusetts':19, 'Michigan':20, 'Minnesota':21, 'Mississippi':22,\n",
    "       'Missouri':23, 'Montana':24, 'Nebraska':25, 'Nevada':26, 'New Hampshire':27,\n",
    "       'New Jersey':28, 'New Mexico':29, 'New York':30, 'North Carolina':31,\n",
    "       'North Dakota':32,'Ohio':33, 'Oklahoma':34,'Oregon':35, 'Pennsylvania':36,\n",
    "       'Rhode Island':37, 'South Carolina':38, 'South Dakota':39, 'Tennessee':40,\n",
    "       'Texas':41, 'Utah':42, 'Vermont':43, 'Virginia':44,\n",
    "       'Washington':45, 'West Virginia':46, 'Wisconsin':47, 'Wyoming':48}\n",
    "\n",
    "    # Use the map() method to apply the mapping to the state column\n",
    "    data['State'] = data['Province_State'].map(state_to_number)\n",
    "    data = data.drop(['Province_State'], axis=1)\n",
    "    data = data.loc[data['State'] == state]\n",
    "  \n",
    "    observed = data[['Date','Deaths']]\n",
    "    data['Deaths'] = data['Deaths'].abs()\n",
    "    observed['Deaths'] = data['Deaths'].abs()\n",
    "    data.reset_index(inplace=True, drop= True)\n",
    "    \n",
    "    observed.rename(columns={'Deaths': 'observed'}, inplace=True)\n",
    "    observed.reset_index(inplace=True, drop= True)\n",
    "   \n",
    "    daily_nn[['week4_GNN','week3_GNN','week2_GNN','week1_GNN']] = daily_nn[['week4_GNN','week3_GNN','week2_GNN','week1_GNN']].apply(lambda x: (x*death_stds)+death_mean)\n",
    "    observed['Date'] = pd.to_datetime(observed['Date'])\n",
    "    matrix = reduce(lambda x,y: pd.merge(x,y, on='Date'), [observed, daily_nn])\n",
    "    matrix = matrix[matrix['observed'] != 0]\n",
    "    matrix.reset_index(inplace=True, drop= True)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0ce6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_style(val):\n",
    "    return \"font-weight: bold\"\n",
    "\n",
    "def smape(actual,forecast): \n",
    "    return 100/len(actual) * np.sum(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast)))\n",
    "\n",
    "def mape(y_true,y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def mae(y_true,y_pred):\n",
    "    error = 0\n",
    "    for i in range(len(y_true)):\n",
    "        error += abs(y_true[i] - y_pred[i])\n",
    "    return (float((error / len(y_true))))\n",
    "\n",
    "def rmse(y_true,y_pred):\n",
    "    mse = sklearn.metrics.mean_squared_error(y_true, y_pred)   \n",
    "    rmse = math.sqrt(mse) \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd7cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_results(matrix):\n",
    "    results_rmse = {}\n",
    "    for i in ('GNN','GNN'):\n",
    "        results_rmse[i] = []\n",
    "        for j in('week1','week2','week3','week4'):\n",
    "            results_rmse[i].append(rmse(matrix['observed'],matrix[j+'_'+i]))\n",
    "    results_rmse = pd.DataFrame(results_rmse)\n",
    "    results_rmse.index += 1 \n",
    "    results_rmse.loc['Average_RMSE'] = results_rmse.mean()\n",
    "    last_row = pd.IndexSlice[results_rmse.index[results_rmse.index == \"Average_RMSE\"], :]\n",
    "    results_rmse.style.applymap(df_style, subset=last_row)\n",
    "    \n",
    "    return results_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec52faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------MAE------------#\n",
    "def mae_results(matrix):\n",
    "    results_mae = {}\n",
    "    for i in ('GNN','GNN'):\n",
    "        results_mae[i] = []\n",
    "        for j in('week1','week2','week3','week4'):\n",
    "            results_mae[i].append(mae(matrix['observed'],matrix[j+'_'+i]))\n",
    "    results_mae = pd.DataFrame(results_mae)\n",
    "    results_mae.index += 1 \n",
    "    results_mae.loc['Average_MAE'] = results_mae.mean()\n",
    "    last_row = pd.IndexSlice[results_mae.index[results_mae.index == \"Average_MAE\"], :]\n",
    "    results_mae.style.applymap(df_style, subset=last_row)\n",
    "    \n",
    "    return results_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03e941d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------SMAPE------------#\n",
    "def smape_results(matrix):\n",
    "    results_smape = {}\n",
    "    for i in ('GNN','GNN'):\n",
    "        results_smape[i] = []\n",
    "        for j in('week1','week2','week3','week4'):\n",
    "            results_smape[i].append(smape(matrix['observed'],matrix[j+'_'+i]))\n",
    "    results_smape = pd.DataFrame(results_smape)\n",
    "    results_smape.index += 1 \n",
    "    results_smape.loc['Average_sMAPE'] = results_smape.mean()\n",
    "    last_row = pd.IndexSlice[results_smape.index[results_smape.index == \"Average_sMAPE\"], :]\n",
    "    results_smape.style.applymap(df_style, subset=last_row)\n",
    "    \n",
    "    return results_smape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c784e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "fix_seed = 1000\n",
    "\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "batch_sizes = [2,4,8,16]\n",
    "hidden_unitss = [32,16,8,4,2]\n",
    "trainratio = 0.8\n",
    "test_ratio = 0.2\n",
    "node_f = 2\n",
    "periods = 4\n",
    "epochs = 100\n",
    "past_values = 6\n",
    "dataset,dates,death_stds,death_mean = data_loader(past_values,periods)\n",
    "for batch_size in batch_sizes:\n",
    "    for hidden_units in hidden_unitss:\n",
    "        model = train(node_f,periods,batch_size,past_values,hidden_units,trainratio,test_ratio,epochs,dataset,dates)\n",
    "        for state in range(49):\n",
    "            forecast_matrix = get_state_result(state, model)\n",
    "            daily_nn = predictions(forecast_matrix)\n",
    "            matrix = get_observed(state,daily_nn)\n",
    "            smape_results_state = smape_results(matrix)\n",
    "            mae_results_state = mae_results(matrix)\n",
    "            rmse_results_state = rmse_results(matrix)\n",
    "            print(\"STATE: \",state)\n",
    "            print(past_values,batch_size,hidden_units)\n",
    "            print(smape_results_state)\n",
    "            print(mae_results_state)\n",
    "            print(rmse_results_state)\n",
    "            print(\"________________________________________\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
